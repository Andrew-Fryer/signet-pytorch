# -*- coding: utf-8 -*-
"""ELEC 874 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VJ_jLeJEA_ZT57oxJwX9BRwU3t1cMm2X

I'm building off of this: https://github.com/VinhLoiIT/signet-pytorch
"""

import sys
print(sys.version)

import os
import numpy as np
import pandas as pd
import itertools
import glob
from PIL import Image, ImageOps
import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchsummary import summary
import matplotlib.pyplot as plt
import datetime

from torch.utils.data import Dataset, DataLoader

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
!mkdir /content/drive/MyDrive/Colab\ Data/
data_dir = '/content/drive/MyDrive/Colab Data/'
# %cd /content/drive/MyDrive/Colab\ Data/
!mkdir cedar_signatures
# %cd cedar_signatures
sig_data_dir = data_dir + 'cedar_signatures/'

def download_data():
    !wget https://cedar.buffalo.edu/NIJ/data/signatures.rar -O signatures.rar
    !unrar x -y signatures.rar
    !mv signatures/* ./
    !rmdir signatures

train_test_csv_file = 'train_test_pairs.csv'
# note that the file names in the dirs are unique...
genuine_dir = sig_data_dir + 'full_org/'
forge_dir = sig_data_dir + 'full_forg/'
num_signers = 55
num_genuine_sign = 24
num_forged_sign = 24
def make_train_and_test_pairs(num_training=50, random_state=0):
    from sklearn.model_selection import train_test_split
    signers = list(range(num_signers))
    train_signers, test_signers = train_test_split(signers, \
        test_size=num_signers-num_training, random_state=random_state)

    import itertools
    # See Section 3.3
    M = num_training
    K = num_signers
    num_testing = K - M
    # 276 (geniune, genuine) signature pairs
    pair_genuine_genuine = list(itertools.combinations(range(num_genuine_sign), 2))
    # 576 (genuine, forged) signature pairs
    pair_genuine_forged = list(itertools.product(range(num_genuine_sign), range(num_forged_sign)))
    
    import random
    random.seed(random_state)

    cols = ['mode', 'signer', 'dir1', 'file1', 'dir2', 'file2', 'label']
    samples = []
    for signer_id in signers:
        mode = 'train' if signer_id in train_signers else 'test'
        for a, b in pair_genuine_genuine:
            samples.append((mode, signer_id, genuine_dir, f'original_{signer_id+1}_{a+1}.png', genuine_dir, f'original_{signer_id+1}_{b+1}.png', 1))
        # randomly trim to 276 by sampling without replacement
        sub_pair_genuine_forged = random.sample(pair_genuine_forged, len(pair_genuine_genuine))
        for a, b in sub_pair_genuine_forged:
            samples.append((mode, signer_id, genuine_dir, f'original_{signer_id+1}_{a+1}.png', forge_dir, f'forgeries_{signer_id+1}_{b+1}.png', 0))
    df = pd.DataFrame(samples, columns=cols)
    print('saving file:', train_test_csv_file)
    df.to_csv(train_test_csv_file)

    return train_signers

def get_df():
    return pd.read_csv(train_test_csv_file)

def compute_mean_and_std_dev(train_signers):
    train_imgs = [genuine_dir + f'original_{signer_id+1}_{i+1}.png' \
        for signer_id in train_signers \
            for i in range(num_genuine_sign)]\
        + [forge_dir + f'forgeries_{signer_id+1}_{i+1}.png' \
            for signer_id in train_signers \
                for i in range(num_forged_sign)]
    no_norm_transform = torchvision.transforms.Compose([
        torchvision.transforms.Resize((155, 220)),
        ImageOps.invert,
        torchvision.transforms.ToTensor(),
    ])
    train_img_data = torch.stack([
        no_norm_transform(Image.open(f).convert('L')) for f in train_imgs
    ], dim=0).numpy()
    mean = train_img_data[:, :, :].mean()
    std_dev = train_img_data[:, :, :].std()
    return mean, std_dev

transform_imgs_dir = sig_data_dir + 'transformed/'
def transform_imgs(mean, std_dev):
    from torchvision.utils import save_image
    image_transform = torchvision.transforms.Compose([
        # "We resize all the images to a fixed size 155Ã—220 using bilinear interpolation."
        torchvision.transforms.Resize((155, 220)), # Default is InterpolationMode.BILINEAR
        # "Afterwards, we invert the images so that the background pixels have 0 values."
        ImageOps.invert,
        torchvision.transforms.ToTensor(),
        # "Furthermore, we normalize each image by dividing the pixel values with the standard deviation of the pixel values of the images in a dataset."
        torchvision.transforms.Normalize(mean, std_dev),
    ])
    img_dir_files = [(genuine_dir, f'original_{signer_id+1}_{i+1}.png') \
        for signer_id in range(num_signers) \
            for i in range(num_genuine_sign)]\
        + [(forge_dir, f'forgeries_{signer_id+1}_{i+1}.png') \
            for signer_id in range(num_signers) \
                for i in range(num_forged_sign)]
    for d, f in img_dir_files:
        pil = Image.open(d + f).convert('L')
        tensor = image_transform(pil)
        save_image(tensor, transform_imgs_dir + f)

def prep_data():
    if not os.path.exists(transform_imgs_dir):
        os.mkdir(transform_imgs_dir)
        if not os.path.exists(sig_data_dir + "signatures.rar"):
            print('downloading data')
            download_data()
        print('splitting data')
        train_signers = make_train_and_test_pairs()
        print('normalizing data')
        mean, std_dev = compute_mean_and_std_dev(train_signers)
        transform_imgs(mean, std_dev)

# 2400 * 155 * 220 ~= 82 MB, so we should be fine to store it all in memory at once...
def load_imgs():
    imgs = []
    file_to_ind = {}
    # transform = torchvision.transforms.ToTensor()
    for f in os.listdir(transform_imgs_dir):
        file = os.path.join(transform_imgs_dir, f)
        # tensor = transform(torchvision.io.read_image(file, torchvision.io.ImageReadMode.GRAY))
        tensor = torchvision.io.read_image(file, torchvision.io.ImageReadMode.GRAY).float()
        file_to_ind[os.path.basename(file)] = len(imgs)
        imgs.append(tensor)
    return imgs, file_to_ind

class SignDataset(Dataset):
    def __init__(self, df, imgs):
        self.df = df
        self.imgs = imgs
    def __len__(self):
        return len(self.df)
    def __getitem__(self, index):
        i1, i2, y = self.df.iloc[index]
        return self.imgs[i1], self.imgs[i2], y

prep_data()
imgs, file_to_ind = load_imgs()
df = get_df()
df['ind1'] = df['file1'].apply(lambda x: file_to_ind[x])
df['ind2'] = df['file2'].apply(lambda x: file_to_ind[x])

train_set = SignDataset(df[df['mode'] == 'train'][['ind1', 'ind2', 'label']], imgs)
val_set = SignDataset(df[df['mode'] == 'test'][['ind1', 'ind2', 'label']], imgs)

# For SimpleNet in the assignment, I had 32 * 32 images and a batch size of 8
# To make batches the same size of data, batch size here should be 0.24
batch_size = 1 # 8 # 32
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True)

seed = 2020
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Device: {}'.format(device))

'''
Reference: https://github.com/VinhLoiIT/signet-pytorch
'''
class SigNet256(nn.Module):
    '''
    Reference Keras: https://github.com/sounakdey/SigNet/blob/master/SigNet_v1.py
    '''
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            #input size = [155, 220, 1]
            nn.Conv2d(1, 96, 11), # size = [145,210]
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [72, 105]
            nn.Conv2d(96, 256, 5, padding=2, padding_mode='zeros'), # size = [72, 105]
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [36, 52]
            nn.Dropout2d(p=0.3),
            nn.Conv2d(256, 384, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.Conv2d(384, 256, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.MaxPool2d(2, stride=2), # size = [18, 26]
            nn.Dropout2d(p=0.3),
            nn.Flatten(1, -1), # 18*26*256
            nn.Linear(18*26*256, 1024),
            nn.Dropout2d(p=0.5),
            nn.Linear(1024, 256),
        )
        self.latent_dims = 256
        self.save_file = sig_data_dir+'progress_256.pt'
    def forward(self, x):
        return self.features(x)
class SigNet128(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            #input size = [155, 220, 1]
            nn.Conv2d(1, 96, 11), # size = [145,210]
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [72, 105]
            nn.Conv2d(96, 256, 5, padding=2, padding_mode='zeros'), # size = [72, 105]
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [36, 52]
            nn.Dropout2d(p=0.3),
            nn.Conv2d(256, 384, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.Conv2d(384, 256, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.MaxPool2d(2, stride=2), # size = [18, 26]
            nn.Dropout2d(p=0.3),
            nn.Flatten(1, -1), # 18*26*256
            nn.Linear(18*26*256, 1024),
            nn.Dropout2d(p=0.5),
            nn.Linear(1024, 128),
        )
        self.latent_dims = 128
        self.save_file = sig_data_dir+'progress_128.pt'
    def forward(self, x):
        return self.features(x)
class SigNet64(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            #input size = [155, 220, 1]
            nn.Conv2d(1, 96, 11), # size = [145,210]
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [72, 105]
            nn.Conv2d(96, 256, 5, padding=2, padding_mode='zeros'), # size = [72, 105]
            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),
            nn.MaxPool2d(2, stride=2), # size = [36, 52]
            nn.Dropout2d(p=0.3),
            nn.Conv2d(256, 384, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.Conv2d(384, 256, 3, stride=1, padding=1, padding_mode='zeros'),
            nn.MaxPool2d(2, stride=2), # size = [18, 26]
            nn.Dropout2d(p=0.3),
            nn.Flatten(1, -1), # 18*26*256
            nn.Linear(18*26*256, 1024),
            nn.Dropout2d(p=0.5),
            nn.Linear(1024, 64),
        )
        self.latent_dims = 64
        self.save_file = sig_data_dir+'progress_64.pt'
    def forward(self, x):
        return self.features(x)

class ContrastiveLoss(nn.Module):
    def __init__(self, alpha, beta, margin):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.margin = margin

    def forward(self, x1, x2, y):
        '''
        Shapes:
        -------
        x1: [B,C]
        x2: [B,C]
        y: [B,1]

        Returns:
        --------
        loss: [B,1]]
        '''
        distance = torch.pairwise_distance(x1, x2, p=2)
        loss = self.alpha * (1-y) * distance**2 + \
               self.beta * y * (torch.max(torch.zeros_like(distance), self.margin - distance)**2)
        return torch.mean(loss, dtype=torch.float)

class SigSystem:
    def __init__(self, model):
        model = model.to(device)
        self.net = model
        self.criterion = ContrastiveLoss(alpha=1, beta=1, margin=1).to(device)
        self.optimizer = optim.RMSprop(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=5e-4, momentum=0.9)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, 5, 0.1)
        self.epoch = 0
        # self.best_training_loss = None
        self.batch = 0

        self.log_interval = 1000

        self.threshold_d = None

        self.train_loader = train_loader
        self.val_loader = val_loader
    def summary(self):
        model = self.net
        summary(model, (1, 155, 220)) # (1, 1, 155, 220)
    def leaky_accuracy(self, distances, y, step=0.01):
        min_threshold_d = min(distances)
        max_threshold_d = max(distances)
        max_acc = 0
        same_id = (y == 1)

        best_threshold_d = None
        for threshold_d in torch.arange(min_threshold_d, max_threshold_d+step, step):
            true_positive = (distances <= threshold_d) & (same_id)
            true_positive_rate = true_positive.sum().float() / same_id.sum().float()
            true_negative = (distances > threshold_d) & (~same_id)
            true_negative_rate = true_negative.sum().float() / (~same_id).sum().float()

            acc = 0.5 * (true_negative_rate + true_positive_rate)
            if acc > max_acc:
                max_acc = acc
                best_threshold_d = threshold_d
        true_positive = ((distances <= best_threshold_d) & (same_id)).sum()
        true_negative = ((distances > best_threshold_d) & (~same_id)).sum()
        false_positive = ((distances <= best_threshold_d) & (~same_id)).sum()
        false_negative = ((distances > best_threshold_d) & (same_id)).sum()
        confusion_matrix = [
            [true_positive.item(), false_negative.item()],
            [false_positive.item(), true_negative.item()],
        ]
        print('confusion matrix for', self.net.__class__.__name__, ':',\
              confusion_matrix)
        print('best leaky accuracy threshold:', best_threshold_d)
        return max_acc, best_threshold_d
    def accuracy(self, distances, y, threshold_d=None):
        if threshold_d == None:
            if self.threshold_d == None:
                self.find_threshold_simple()
            threshold_d = self.threshold_d
        same_id = (y == 1)
        true_positive = ((distances <= threshold_d) & (same_id)).sum()
        true_negative = ((distances > threshold_d) & (~same_id)).sum()
        false_positive = ((distances <= threshold_d) & (~same_id)).sum()
        false_negative = ((distances > threshold_d) & (same_id)).sum()
        confusion_matrix = [
            [true_positive.item(), false_negative.item()],
            [false_positive.item(), true_negative.item()],
        ]
        print('confusion matrix for', self.net.__class__.__name__, ':',\
              confusion_matrix)

        num_positive = same_id.sum().float()
        num_negative = (~same_id).sum().float()
        true_positive_rate = true_positive / num_positive if num_positive > 0 else 0
        true_negative_rate = true_negative / num_negative if num_negative > 0 else 0
        acc = 0.5 * (true_negative_rate + true_positive_rate)
        return acc
    # def find_threshold(num_batches=1000):
    #     latent_vector_size = self.net.latent_dims
    #     with torch.no_grad():
    #         same_ids = []
    #         deltas = []
    #         for batch_idx, (x1, x2, y) in enumerate(self.train_loader):
    #             if batch_idx > num_batches:
    #                 break
    #             x1, x2, y = x1.to(device), x2.to(device), y.to(device)
    #             same_id = (y == 1)
    #             same_ids.append(same_id.reshape((-1, latent_vector_size))) #.cpu()

    #             x1, x2 = model(x1), model(x2)
    #             delta = torch.sub(x2, x1)
    #             deltas.append(delta.reshape((-1, latent_vector_size))) #.cpu()
    #         same_ids = torch.stack(same_ids)
    #         deltas = torch.stack(deltas)
    #         deltas_same = deltas[same_ids]
    #         deltas_diff = deltas[~same_ids]
    #         std_dev_same = torch.cov(deltas_same)
    #         std_dev_diff = torch.cov(deltas_diff)
    #         torch.cov(deltas_same)
    #         self.threshold_stds = (std_dev_same, std_dev_diff)
    #         # then, assume that everything is gaussian
    #         #     and threshold is a comparison of gaussian probabilities
    def find_threshold_simple(self, num_batches=1000):
        model = self.net
        with torch.no_grad():
            model.eval()
            i = 0
            distance_data = []
            for batch_idx, (x1, x2, y) in enumerate(train_loader):
                if batch_idx > num_batches:
                    break

                same_id = (y == 1)
                x1, x2, y = x1.to(device), x2.to(device), y.to(device)
                x1, x2 = model(x1), model(x2)
                distance_data.extend(zip(torch.pairwise_distance(x1, x2, 2).cpu().tolist(), y.cpu().tolist()))
            distances, y = zip(*distance_data)
            distances, y = torch.tensor(distances), torch.tensor(y)
            same_id = (y == 1)
            median_positive = distances[same_id].quantile(0.5)
            median_negative = distances[~same_id].quantile(0.5)
            print('medians:', median_positive, median_negative)
            self.threshold_d = (median_positive + median_negative) / 2
    def distance(self, img1, img2):
        pass
    def classify(self):
        pass
    def train(self):
        print('Training', '-'*20)
        model = self.net

        running_loss = 0
        number_samples = 0
        model.train()
        for batch_idx, (x1, x2, y) in enumerate(self.train_loader):
            if batch_idx < self.batch:
                continue
            else:
                self.batch = batch_idx

            x1, x2, y = x1.to(device), x2.to(device), y.to(device)

            self.optimizer.zero_grad()
            x1, x2 = model(x1), model(x2)
            loss = self.criterion(x1, x2, y)
            loss.backward()
            self.optimizer.step()

            number_samples += len(x1)
            running_loss += loss.item() * len(x1)
            if (batch_idx+1) % self.log_interval == 0 or batch_idx == len(self.train_loader) - 1:
                training_loss = running_loss / number_samples
                print('{}/{}: Loss: {:.4f}'.format(batch_idx, len(self.train_loader), training_loss))

                # if best_training_loss == None or training_loss <= best_training_loss:
                #     best_training_loss = training_loss
                #     print('best training loss:', best_training_loss)
                #     self.save()
                running_loss = 0
                number_samples = 0
        self.batch = 0
    def do_training(self, num_epochs=20):
        # self.load()

        for epoch in range(self.epoch, self.epoch + num_epochs):
            print('Epoch {}/{}'.format(epoch, num_epochs))
            self.train()
            self.epoch = epoch
            # self.save() #('_epoch_' + str(epoch))

            # val_loss, acc = self.evaluate()
            # print('epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(epoch, val_loss, acc))
            self.scheduler.step()
        self.epoch = 0
        print('Done training')
    def evaluate(self, improve_forgery=False, leaky_accuracy=False, imp_neg=False):
        print('Evaluating', '-'*20)
        # with torch.no_grad():
        model = self.net
        dataloader = self.val_loader

        model.eval()
        running_loss = 0
        number_samples = 0

        distances = []
        improved_forge_distances = []

        for batch_idx, (x1, x2, y) in enumerate(dataloader):
            x1, x2, y = x1.to(device), x2.to(device), y.to(device)

            if improve_forgery:
                # todo: I could avoid recomputing some latent vectors here
                diff_id = (y != 1)
                diff_x1 = x1.clone()[diff_id]
                diff_x2 = x2.clone()[diff_id]
                imp_x2, _, diff_x1, _ = self.improve_forge(diff_x1, diff_x2) if not imp_neg else self.improve_forge_neg(diff_x1, diff_x2)
                # imp_x2 = imp_x2.to(device)
                imp_x2 = model(imp_x2)
                improved_forge_distances.extend(torch.pairwise_distance(diff_x1, imp_x2, 2).cpu().tolist())
            x1 = model(x1)
            x2 = model(x2)
            loss = self.criterion(x1, x2, y)
            distances.extend(zip(torch.pairwise_distance(x1, x2, 2).cpu().tolist(), y.cpu().tolist()))

            number_samples += len(x1)
            running_loss += loss.item() * len(x1)

            if (batch_idx + 1) % self.log_interval == 0 or batch_idx == len(dataloader) - 1:
                print('{}/{}: Loss: {:.4f}'\
                        .format(batch_idx+1, len(dataloader), running_loss / number_samples))

        distances, y = zip(*distances)
        distances, y = torch.tensor(distances), torch.tensor(y)
        print('computing accuracy of normal pairs...')
        acc = None
        leaky_threshold_d = None
        if not leaky_accuracy:
            acc = self.accuracy(distances, y)
        else:
            acc, leaky_threshold_d = self.leaky_accuracy(distances, y)
        print('Accuracy:', acc)
        imp_distances = None
        if improve_forgery:
            print('computing accuracy of improved forges...')
            imp_distances = torch.tensor(improved_forge_distances)
            imp_y = torch.tensor([0] * len(improved_forge_distances))
            imp_acc = self.accuracy(imp_distances, imp_y, threshold_d=leaky_threshold_d)
            print(f'Accuracy on improved forges: {imp_acc}')
        val_loss = running_loss / number_samples
        print('val loss:', val_loss)
        return val_loss, acc, distances, y, imp_distances, imp_y
    def save(self, tag=""):
        print('Saving checkpoint..', tag)
        save_file = self.net.save_file + tag
        to_save = {
            'model': self.net.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epoch': self.epoch,
            'batch': self.batch,
            # 'training_loss': self.training_loss,
            'threshold_d': self.threshold_d,
        }
        torch.save(to_save, save_file)
    def load(self, tag=""):
        save_file = self.net.save_file + tag
        if os.path.exists(save_file):
            print('loading from checkpoint')
            checkpoint = torch.load(save_file)
            self.net.load_state_dict(checkpoint['model'])
            self.scheduler.load_state_dict(checkpoint['scheduler'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epoch = checkpoint['epoch']
            self.batch = checkpoint['batch']
            # self.best_training_loss = checkpoint['training_loss']

            self.threshold_d = checkpoint.get('threshold_d')
    def improve_forge(self, genuines, forges, eps=0.004, same=1):
        model = self.net
        model.eval() # set training flag to false to avoid batch norm and/or dropout
        forges = forges.to(device)
        genuines = genuines.to(device)
        forges.requires_grad = True
    
        target_outputs = model(genuines) # todo: cache these?
        outputs = model(forges)

        model.zero_grad()
        loss = self.criterion(outputs, target_outputs, same).to(device)
        loss.backward() # I guess this populates the gradients?

        perturbations = eps*forges.grad
        improved_forges = forges + perturbations
        improved_forges = torch.clamp(improved_forges, 0, 1)

        return improved_forges, perturbations, target_outputs, outputs
    def improve_forge_neg(self, genuines, forges, eps=0.004, same=1):
        model = self.net
        model.eval() # set training flag to false to avoid batch norm and/or dropout
        forges = forges.to(device)
        genuines = genuines.to(device)
        forges.requires_grad = True
    
        target_outputs = model(genuines) # todo: cache these?
        outputs = model(forges)

        model.zero_grad()
        loss = self.criterion(outputs, target_outputs, same).to(device)
        loss.backward() # I guess this populates the gradients?

        perturbations = eps*forges.grad.sign()
        improved_forges = forges + perturbations
        improved_forges = torch.clamp(improved_forges, 0, 1)

        return improved_forges, perturbations, target_outputs, outputs

# This (will take many hours and then) generates the main results
systems = [
    SigSystem(SigNet64()),
    SigSystem(SigNet128()),
    SigSystem(SigNet256()),
]
for s in systems:
    s.summary()
    s.load()
    s.do_training()
    s.find_threshold_simple()
    s.save()
    s.evaluate(improve_forgery=True, leaky_accuracy=False, imp_neg=True)
    s.evaluate(improve_forgery=True, leaky_accuracy=True, imp_neg=True)

"""# **Beware**:

---


The below cells are leftover from many small tests and will not run without some coercion!
"""





systems = [
    SigSystem(SigNet64()),
    SigSystem(SigNet128()),
    SigSystem(SigNet256()),
]
for s in systems:
    s.summary()
    s.load()
    # s.do_training()
    # s.find_threshold_simple()
    s.evaluate(improve_forgery=True)

systems = [
    SigSystem(SigNet64()),
    SigSystem(SigNet128()),
    SigSystem(SigNet256()),
]

# systems[0].do_training(num_epochs=1)
systems[1].do_training(num_epochs=1)
systems[2].do_training(num_epochs=1)
systems[1].do_training(num_epochs=1)
systems[1].do_training(num_epochs=1)

# del systems
# del s
torch.cuda.empty_cache()

SigSystem(SigNet64()).do_training()
torch.cuda.empty_cache()

s = SigSystem(SigNet128())
s.do_training()

s.do_training(num_epochs=19)

s.epoch = 5
s.save()

del s
torch.cuda.empty_cache()

s = SigSystem(SigNet256())
s.do_training(num_epochs=5)

s.evaluate(improve_forgery=True)

s.evaluate()

s = SigSystem(SigNet256())
s.load()
s.evaluate(improve_forgery=True)

# del s
torch.cuda.empty_cache()

s = SigSystem(SigNet128())
s.load('_epoch_4')
s.find_threshold_simple()
s.save('_epoch_4')
s.evaluate(improve_forgery=True)

s = SigSystem(SigNet64())
s.load(tag='_epoch_19')
s.evaluate(improve_forgery=True)

s.find_threshold_simple()
s.evaluate(improve_forgery=True)

s = SigSystem(SigNet64())
s.do_training(num_epochs=5)
s.find_threshold_simple()
s.save('_epoch_4')
s.evaluate(improve_forgery=True)

s = SigSystem(SigNet128())
s.load('_epoch_4')
s.do_training(num_epochs=15)
s.save('_epoch_19')
s.find_threshold_simple()
s.save('_epoch_19')
s.evaluate(improve_forgery=True)

del s
torch.cuda.empty_cache()

s = SigSystem(SigNet256())
s.load('_epoch_4')
s.do_training(num_epochs=15)
s.save('_epoch_19')
s.find_threshold_simple()
s.save('_epoch_19')
s.evaluate(improve_forgery=True)

del s
torch.cuda.empty_cache()

s = SigSystem(SigNet128())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=True)

plt.plot(result[2])

plt.hist(result[2], bins=100)

plt.plot(result[3])

plt.hist(result[3], bins=100)

s = SigSystem(SigNet128())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=True)

val_loss, acc, distances, y, imp_distances, imp_y = result
same_id = y == 1
genuine_distances = distances[same_id]
forge_distances = distances[~same_id]

plt.figure(figsize=(8,6))
plt.hist(genuine_distances, bins=100, alpha=0.5, label="Genuine")
plt.hist(forge_distances, bins=100, alpha=0.5, label="Forge")
plt.hist(imp_distances, bins=100, alpha=0.5, label="Perturbed Forge")
plt.xlabel("Distance Between Latent Vectors", size=14)
plt.ylabel("Count", size=14)
# plt.title("Multiple Histograms with Matplotlib")
plt.legend(loc='upper right')
plt.savefig("distance_histogram_signet_128_leaky.png")
plt.show()

s = SigSystem(SigNet128())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=False)

val_loss, acc, distances, y, imp_distances, imp_y = result
same_id = y == 1
genuine_distances = distances[same_id]
forge_distances = distances[~same_id]

plt.figure(figsize=(8,6))
plt.hist(genuine_distances, bins=100, alpha=0.5, label="Genuine")
plt.hist(forge_distances, bins=100, alpha=0.5, label="Forge")
plt.hist(imp_distances, bins=100, alpha=0.5, label="Perturbed Forge")
plt.xlabel("Distance Between Latent Vectors", size=14)
plt.ylabel("Count", size=14)
# plt.title("Multiple Histograms with Matplotlib")
plt.legend(loc='upper right')
plt.savefig("distance_histogram_signet_128.png")
plt.show()

s = SigSystem(SigNet64())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=True)
s.threshold_d

s = SigSystem(SigNet128())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=True)
s.threshold_d

s = SigSystem(SigNet256())
s.load('_epoch_19')
result = s.evaluate(improve_forgery=True, leaky_accuracy=True)
s.threshold_d



s = SigSystem(SigNet64())
s.load('_epoch_19')
s.find_threshold_simple()
s.save('_epoch_19')
result = s.evaluate(improve_forgery=True, imp_neg=True)

s = SigSystem(SigNet128())
s.load('_epoch_19')
s.find_threshold_simple()
s.save('_epoch_19')
result = s.evaluate(improve_forgery=True, imp_neg=True)

val_loss, acc, distances, y, imp_distances, imp_y = result
same_id = y == 1
genuine_distances = distances[same_id]
forge_distances = distances[~same_id]

plt.figure(figsize=(8,6))
plt.hist(genuine_distances, bins=100, alpha=0.5, label="Genuine")
plt.hist(forge_distances, bins=100, alpha=0.5, label="Forge")
plt.hist(imp_distances, bins=100, alpha=0.5, label="Perturbed Forge")
plt.xlabel("Distance Between Latent Vectors", size=14)
plt.ylabel("Count", size=14)
# plt.title("Multiple Histograms with Matplotlib")
plt.legend(loc='upper right')
plt.savefig("distance_histogram_signet_128.png")
plt.show()

files.download("distance_histogram_signet_128.png")

s = SigSystem(SigNet256())
s.load('_epoch_19')
s.find_threshold_simple()
s.save('_epoch_19')
result = s.evaluate(improve_forgery=True, imp_neg=True)

s = SigSystem(SigNet64())
s.load('_epoch_4')
s.find_threshold_simple()
s.save('_epoch_4')
result = s.evaluate(improve_forgery=True, imp_neg=True)

s = SigSystem(SigNet128())
s.load('_epoch_4')
s.find_threshold_simple()
s.save('_epoch_4')
result = s.evaluate(improve_forgery=True, imp_neg=True)

s = SigSystem(SigNet256())
s.load('_epoch_4')
s.find_threshold_simple()
s.save('_epoch_4')
result = s.evaluate(improve_forgery=True, imp_neg=True)



from google.colab import files

files.download(sig_data_dir + 'distance_histogram_signet_128_leaky.png')

files.download(sig_data_dir + 'distance_histogram_signet_128.png')

t = torch.cuda.get_device_properties(0).total_memory
r = torch.cuda.memory_reserved(0)
a = torch.cuda.memory_allocated(0)
f = r-a  # free inside reserved
t, r, a, f



s.threshold_d

# do_training()

# save_file = sig_data_dir+'progress.pt'
# checkpoint = torch.load(save_file)
# # checkpoint['training_loss'] = checkpoint['loss']
# # del checkpoint['loss']
# torch.save(checkpoint, save_file)

save_file = sig_data_dir+'progress.pt' + '_epoch_0'
checkpoint = torch.load(save_file)
model = SigNet().to(device)
model.load_state_dict(checkpoint['model'])
criterion = ContrastiveLoss(alpha=1, beta=1, margin=1).to(device)





torch.cuda.empty_cache()





genuine = imgs[file_to_ind['original_1_1.png']]
plt.imshow(genuine.numpy().reshape((155, 220)))

forge = imgs[file_to_ind['forgeries_1_1.png']]
plt.imshow(forge.numpy().reshape((155, 220)))

genuine

forge

(genuine < 1).sum()

(genuine < 2).sum()

(genuine < 3).sum()

(genuine < 4).sum()

plt.hist(genuine.reshape(-1), bins=range(0, 256))

plt.hist(forge.reshape(-1), bins=range(0, 256))

genuine.int()

torch.bincount(genuine.reshape(-1).int())

torch.bincount(forge.reshape(-1).int())

x2_bincounts = []
ys = []
for batch_idx, (x1, x2, y) in enumerate(val_loader):
    x2_bincounts.append(torch.bincount(x2.reshape(-1).int(), minlength=256))
    ys.append(y)
y = torch.stack(ys).reshape(-1)
x2 = torch.stack(x2_bincounts).reshape((-1, 256)).float()
same_id = y == 1
genuine_mean = torch.mean(x2[same_id], dim=0)
forge_mean = torch.mean(x2[~same_id], dim=0)

plt.scatter(range(256), genuine_mean, alpha=0.5, marker='.')
plt.scatter(range(256), forge_mean, alpha=0.5, marker='x')
plt.legend(['Genuine', 'Forge'])
save_file = sig_data_dir + 'mean_hist.png'
plt.savefig(save_file)
files.download(save_file)



plt.scatter(range(256), [genuine_mean, forge_mean])

def heuristic_classifier(x2):
    return torch.logical_and(0 < x2, x2 < 255).sum() > 5000

num_correct = 0
total_num = 0
false_positives = 0
for batch_idx, (x1, x2, y) in enumerate(val_loader):
    prediction = heuristic_classifier(x2)
    correct = prediction == y
    false_positives += torch.logical_and(prediction, ~correct).sum()
    num_correct += correct.sum()
    total_num += len(y)
print(total_num, num_correct, false_positives, total_num - num_correct)

forge_mean

genuine_mean



model = s.net
model

genuines = torch.reshape(genuine, (1, 1, 155, 220))
forges = torch.reshape(forge, (1, 1, 155, 220))
improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, forges)

perturbations[0][0]

plt.imshow(perturbations[0][0].cpu(), cmap='gray')
plt.savefig('perturbation.png')
files.download('perturbation.png')

genuines = genuines.to(device)
forges = forges.to(device)
genuine_forge_dist = torch.pairwise_distance(model(genuines), model(forges), 2).item()

genuine_forge_dist

genuine_improved_forge_dist = torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

genuine_improved_forge_dist

s.threshold_d

plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')

plt.imshow(-improved_forge.cpu().detach().numpy().reshape((155, 220)) + forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')

improved_forge.cpu() - forge

forge.max()

improved_forge[improved_forge < 1.0e-06] = 0.0

improved_forge

genuines = genuines.to(device)

torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

(improved_forge.cpu() - forge).max()

(forge).max()

plt.imshow((improved_forge.detach().cpu() - forge.cpu()).reshape((155, 220)), cmap='gray')

(improved_forge.cpu() - forge.cpu()).reshape((155, 220))

plt.imshow(np.zeros((155, 220)), cmap='gray')

std_dev = 0.07968917
noise = torch.rand_like(improved_forge.cpu()) * std_dev
noisy_forge = forge + noise

plt.imshow(noise.reshape((155, 220)), cmap='gray')

plt.imshow(noisy_forge.reshape((155, 220)), cmap='gray')

noisy_forge.reshape((155, 220))

noise = noise.to(device)
torch.pairwise_distance(model(genuines), model(noise), 2).item()

torch.pairwise_distance(model(genuines), model(noisy_forge.to(device)), 2).item()

torch.pairwise_distance(model(genuines), model(torch.rand_like(improved_forge) * 255), 2).item()

model.eval()



improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()

improved_forge = torch.autograd.Variable(improved_forge, requires_grad=False)
improved_forge = improved_forge, perturbations, target_outputs, outputs = s.improve_forge_neg(genuines, improved_forge)
plt.imshow(improved_forge.cpu().detach().numpy().reshape((155, 220)), cmap='gray')
torch.pairwise_distance(model(genuines), model(improved_forge), 2).item()